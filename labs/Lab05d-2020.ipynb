{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab5d: PCA with text 2/21/19\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Imagining Communities using texts\n",
    "\n",
    "1. Using words as features: Words as Vectors in unique dimensions\n",
    "2. Imagining political communties / political discourse\n",
    "3. Try this at home: PCA on your own texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some libraries we'll need...\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA as sklearnPCA \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "##import text_analysis as ta \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagining communities via their textual traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using words as features\n",
    "We can use PCA to examine collections of texts. To do this, we treat each word as a feature.\n",
    "\n",
    "Thus for a corpus with just two texts of two words each, namely, $T_1 = (the, the)$ and $T_2 = (the, moose)$, we would draw:\n",
    "\n",
    "![Words](fig/Words.jpeg)\n",
    "\n",
    "For each additional text $T_i$, we'd add a new data point in the plot above. For each additional *word* in the corpus, we'd add a new dimension to the plot above. If our corpus has a million words, our plot above would have a million dimensions. As above, we'll use PCA to reduce a high-dimension data set into a 2-dimension data set that we can easily visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a small sample of tweets: US politicians (and their offices) for 1 day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtag_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>825817400465571841</td>\n",
       "      <td>76649729</td>\n",
       "      <td>2017-01-29 21:26:25</td>\n",
       "      <td>RT @GOPHELP: Betsy DeVos says in letter - \"I a...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825725985458880514</td>\n",
       "      <td>76649729</td>\n",
       "      <td>2017-01-29 15:23:10</td>\n",
       "      <td>RT @GOPHELP: Op-ed in @memphisnews:\"DeVos has ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>825839671208972290</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 22:54:55</td>\n",
       "      <td>RT @JCColtin: @RepEspaillat inauguration going...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>825820208736718850</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 21:37:35</td>\n",
       "      <td>RT @NYCMayor: We're the safest big city in Ame...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825820131196620800</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 21:37:16</td>\n",
       "      <td>RT @NYCMayor: I'll be joining New Yorkers at B...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>825820092684496903</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 21:37:07</td>\n",
       "      <td>RT @NYCMayor: There is something more importan...</td>\n",
       "      <td>{BatteryPark}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>825819737217183745</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 21:35:42</td>\n",
       "      <td>RT @LizRittr: .@BilldeBlasio to @jeffsessions:...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>825819634402213888</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 21:35:18</td>\n",
       "      <td>Thank you @SenSchumer for your remarks today @...</td>\n",
       "      <td>{NY13}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>825808721288822784</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:51:56</td>\n",
       "      <td>RT @jonorcutt: Great rallying cries from @RepE...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>825808666012090368</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:51:43</td>\n",
       "      <td>RT @altochulo: No justice, no peace! cries @Re...</td>\n",
       "      <td>{NoBanNoWall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>825808543769096192</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:51:14</td>\n",
       "      <td>RT @LizRittr: Packed house waiting for the com...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>825808514748645376</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:51:07</td>\n",
       "      <td>RT @KariStrong: Here to support and celebrate ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>825808474353311744</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:50:57</td>\n",
       "      <td>RT @angelabaggetta: Packed house for the cerem...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>825808449783078912</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:50:51</td>\n",
       "      <td>RT @javierhvaldes: Nuestro hermano @RepEspaill...</td>\n",
       "      <td>{HeretoStay,NoBanNoWall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>825808398088298497</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:50:39</td>\n",
       "      <td>RT @KelleenLib: Thank you @RepEspaillat 4 bein...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>825808388470820865</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:50:37</td>\n",
       "      <td>RT @ZaharaMZ: .@RepEspaillat calls out NYC wea...</td>\n",
       "      <td>{NoBanNoWall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>825808189434298368</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:49:49</td>\n",
       "      <td>RT @rebeccalounyc: @RepYvetteClarke @GregoryMe...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>825808131095662592</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 20:49:35</td>\n",
       "      <td>RT @marializardo: @RepEspaillat Getting ready ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>825528080139698176</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 02:16:46</td>\n",
       "      <td>A Federal Judge Just Issued A Stay Against Don...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>825508823435710464</td>\n",
       "      <td>817076257770835968</td>\n",
       "      <td>2017-01-29 01:00:15</td>\n",
       "      <td>@RepEspaillat @GregoryMeeks #JFK airport talki...</td>\n",
       "      <td>{JFK}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>825796597871153156</td>\n",
       "      <td>140519774</td>\n",
       "      <td>2017-01-29 20:03:46</td>\n",
       "      <td>Finding Dory? Maybe @realDonaldTrump should fo...</td>\n",
       "      <td>{StandWithRefugees}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>825737055686840320</td>\n",
       "      <td>140519774</td>\n",
       "      <td>2017-01-29 16:07:10</td>\n",
       "      <td>\"History will remember them as cowards.\" Here'...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>825733792233054208</td>\n",
       "      <td>140519774</td>\n",
       "      <td>2017-01-29 15:54:12</td>\n",
       "      <td>Some good news, but we're not done fighting ye...</td>\n",
       "      <td>{StandWithRefugees}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>825728806468059136</td>\n",
       "      <td>140519774</td>\n",
       "      <td>2017-01-29 15:34:23</td>\n",
       "      <td>RT @NancyPelosi: We believe health care is a r...</td>\n",
       "      <td>{ProtectOurCare}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>825822077156470785</td>\n",
       "      <td>236916916</td>\n",
       "      <td>2017-01-29 21:45:00</td>\n",
       "      <td>My statement regarding President Trump's execu...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>825694188989771777</td>\n",
       "      <td>235373000</td>\n",
       "      <td>2017-01-29 13:16:49</td>\n",
       "      <td>Yes!  Even US allies recognize the error in US...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>825580348608225281</td>\n",
       "      <td>235373000</td>\n",
       "      <td>2017-01-29 05:44:28</td>\n",
       "      <td>The American public has a right to know how @D...</td>\n",
       "      <td>{NoBanOnRefugees}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>825561136749744128</td>\n",
       "      <td>235373000</td>\n",
       "      <td>2017-01-29 04:28:07</td>\n",
       "      <td>Thanks to the ACLU for standing up to the Trum...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>825855570322276357</td>\n",
       "      <td>558769636</td>\n",
       "      <td>2017-01-29 23:58:06</td>\n",
       "      <td>Turning our backs on refugees\\nbetrays our Ame...</td>\n",
       "      <td>{nobanpdx}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>825834152985501701</td>\n",
       "      <td>558769636</td>\n",
       "      <td>2017-01-29 22:32:59</td>\n",
       "      <td>RT @tedwheeler: At PDX w/ @SenJeffMerkley and ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>825569312341635072</td>\n",
       "      <td>7429102</td>\n",
       "      <td>2017-01-29 05:00:37</td>\n",
       "      <td>@soltanlife on it. Huge, troubling problem. De...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>825568412898361345</td>\n",
       "      <td>7429102</td>\n",
       "      <td>2017-01-29 04:57:02</td>\n",
       "      <td>@RossSchulman working on this as we speak. Hug...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>825510674902179840</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 01:07:36</td>\n",
       "      <td>RT @MaketheRoadNY: .@NydiaVelazquez @RepJerryN...</td>\n",
       "      <td>{MuslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>825510439333269504</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 01:06:40</td>\n",
       "      <td>RT @HeyItsMurad: JOIN US! They finally let one...</td>\n",
       "      <td>{JFK,T4,NoBanNoWall,MuslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>825827569199087617</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 22:06:50</td>\n",
       "      <td>RT @MaketheRoadNY: yesterday @NydiaVelazquez h...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>825509566846738433</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 01:03:12</td>\n",
       "      <td>RT @DanaGpdx: Thank you congresswoman @NydiaVe...</td>\n",
       "      <td>{MuslimBan,RefugeesWelcome}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>825774121883676672</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 18:34:27</td>\n",
       "      <td>Proud to join Brooklyn Chinese-American Associ...</td>\n",
       "      <td>{LunarNewYear}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>825751377238450178</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 17:04:04</td>\n",
       "      <td>RT @RepJerryNadler: Video from yesterday: @Nyd...</td>\n",
       "      <td>{MuslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>825799292766019585</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 20:14:28</td>\n",
       "      <td>Good rally today in support of refugees! https...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>825799094148984834</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 20:13:41</td>\n",
       "      <td>Speaking today at rally against unconstitution...</td>\n",
       "      <td>{NoWallNoBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>825774711481188352</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 18:36:48</td>\n",
       "      <td>Speaking to friends at Brooklyn Chinese-Americ...</td>\n",
       "      <td>{yearoftherooster}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>825732902583463936</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 15:50:40</td>\n",
       "      <td>Fed Courts Must Overturn Trump's EO! Immigrati...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>825728324907520001</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 15:32:28</td>\n",
       "      <td>Yesterday's court stay was victory but we must...</td>\n",
       "      <td>{NoWallNoBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>825510364565606401</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 01:06:22</td>\n",
       "      <td>RT @HeyItsMurad: Hameed Darwish, #Iraqi #Refug...</td>\n",
       "      <td>{Iraqi,Refugee,NoBanNoWall,muslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>825498115746754561</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 00:17:42</td>\n",
       "      <td>RT @JackSmithIV: Inside JFK, fighting for the ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>825495311967780865</td>\n",
       "      <td>164369297</td>\n",
       "      <td>2017-01-29 00:06:33</td>\n",
       "      <td>RT @NydiaVelazquez: At #JFK with @RepJerryNadl...</td>\n",
       "      <td>{JFK}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>825781814157508608</td>\n",
       "      <td>1072467470</td>\n",
       "      <td>2017-01-29 19:05:01</td>\n",
       "      <td>I'm committed to keeping our river clean and s...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>825772454928842752</td>\n",
       "      <td>1072467470</td>\n",
       "      <td>2017-01-29 18:27:50</td>\n",
       "      <td>My statement on the #MuslimBan: https://t.co/G...</td>\n",
       "      <td>{MuslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>825754131864383493</td>\n",
       "      <td>1072467470</td>\n",
       "      <td>2017-01-29 17:15:01</td>\n",
       "      <td>Interested in what's happening in Washington? ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>825828584489099264</td>\n",
       "      <td>104198706</td>\n",
       "      <td>2017-01-29 22:10:52</td>\n",
       "      <td>Virginians deserve answers about the implement...</td>\n",
       "      <td>{MuslimBan}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>825561879959523329</td>\n",
       "      <td>104198706</td>\n",
       "      <td>2017-01-29 04:31:05</td>\n",
       "      <td>Welcome to Virginia! https://t.co/YdsymB5N4h</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>825843236086374400</td>\n",
       "      <td>22523087</td>\n",
       "      <td>2017-01-29 23:09:05</td>\n",
       "      <td>RT @MayorMeganBarry: Glad to see @SenBobCorker...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>825757776240640001</td>\n",
       "      <td>22523087</td>\n",
       "      <td>2017-01-29 17:29:30</td>\n",
       "      <td>RT @ABCPolitics: Former Defense Sec. Bob Gates...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>825751005648261121</td>\n",
       "      <td>22523087</td>\n",
       "      <td>2017-01-29 17:02:36</td>\n",
       "      <td>RT @politico: McCain blasts Bannon placement o...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>825854237288648704</td>\n",
       "      <td>1140648348</td>\n",
       "      <td>2017-01-29 23:52:48</td>\n",
       "      <td>#LillyLedbetter Fair Pay Act signed 8 yrs ago ...</td>\n",
       "      <td>{LillyLedbetter,EqualPay}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>825787950713729025</td>\n",
       "      <td>1140648348</td>\n",
       "      <td>2017-01-29 19:29:24</td>\n",
       "      <td>We weaken our nation by turning our back on th...</td>\n",
       "      <td>{NoBanNoWall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>825704463289688064</td>\n",
       "      <td>787373558</td>\n",
       "      <td>2017-01-29 13:57:39</td>\n",
       "      <td>RT @OversightDems: .@RepCummings: 17 Intel Age...</td>\n",
       "      <td>{RussianHacking,Election2016}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>825704452300619776</td>\n",
       "      <td>787373558</td>\n",
       "      <td>2017-01-29 13:57:36</td>\n",
       "      <td>RT @OversightDems: .@RepCummings: #Trump: if y...</td>\n",
       "      <td>{Trump,voterfraud}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>825704443794563072</td>\n",
       "      <td>787373558</td>\n",
       "      <td>2017-01-29 13:57:34</td>\n",
       "      <td>RT @OversightDems: .@RepCummings: This #voterf...</td>\n",
       "      <td>{voterfraud}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>825521962164289536</td>\n",
       "      <td>237770636</td>\n",
       "      <td>2017-01-29 01:52:27</td>\n",
       "      <td>We shouldn't impose a religious test on anyone...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id             user_id           created_at  \\\n",
       "0     825817400465571841            76649729  2017-01-29 21:26:25   \n",
       "1     825725985458880514            76649729  2017-01-29 15:23:10   \n",
       "2     825839671208972290  817076257770835968  2017-01-29 22:54:55   \n",
       "3     825820208736718850  817076257770835968  2017-01-29 21:37:35   \n",
       "4     825820131196620800  817076257770835968  2017-01-29 21:37:16   \n",
       "5     825820092684496903  817076257770835968  2017-01-29 21:37:07   \n",
       "6     825819737217183745  817076257770835968  2017-01-29 21:35:42   \n",
       "7     825819634402213888  817076257770835968  2017-01-29 21:35:18   \n",
       "8     825808721288822784  817076257770835968  2017-01-29 20:51:56   \n",
       "9     825808666012090368  817076257770835968  2017-01-29 20:51:43   \n",
       "10    825808543769096192  817076257770835968  2017-01-29 20:51:14   \n",
       "11    825808514748645376  817076257770835968  2017-01-29 20:51:07   \n",
       "12    825808474353311744  817076257770835968  2017-01-29 20:50:57   \n",
       "13    825808449783078912  817076257770835968  2017-01-29 20:50:51   \n",
       "14    825808398088298497  817076257770835968  2017-01-29 20:50:39   \n",
       "15    825808388470820865  817076257770835968  2017-01-29 20:50:37   \n",
       "16    825808189434298368  817076257770835968  2017-01-29 20:49:49   \n",
       "17    825808131095662592  817076257770835968  2017-01-29 20:49:35   \n",
       "18    825528080139698176  817076257770835968  2017-01-29 02:16:46   \n",
       "19    825508823435710464  817076257770835968  2017-01-29 01:00:15   \n",
       "20    825796597871153156           140519774  2017-01-29 20:03:46   \n",
       "21    825737055686840320           140519774  2017-01-29 16:07:10   \n",
       "22    825733792233054208           140519774  2017-01-29 15:54:12   \n",
       "23    825728806468059136           140519774  2017-01-29 15:34:23   \n",
       "24    825822077156470785           236916916  2017-01-29 21:45:00   \n",
       "25    825694188989771777           235373000  2017-01-29 13:16:49   \n",
       "26    825580348608225281           235373000  2017-01-29 05:44:28   \n",
       "27    825561136749744128           235373000  2017-01-29 04:28:07   \n",
       "28    825855570322276357           558769636  2017-01-29 23:58:06   \n",
       "29    825834152985501701           558769636  2017-01-29 22:32:59   \n",
       "...                  ...                 ...                  ...   \n",
       "1153  825569312341635072             7429102  2017-01-29 05:00:37   \n",
       "1154  825568412898361345             7429102  2017-01-29 04:57:02   \n",
       "1155  825510674902179840           164369297  2017-01-29 01:07:36   \n",
       "1156  825510439333269504           164369297  2017-01-29 01:06:40   \n",
       "1157  825827569199087617           164369297  2017-01-29 22:06:50   \n",
       "1158  825509566846738433           164369297  2017-01-29 01:03:12   \n",
       "1159  825774121883676672           164369297  2017-01-29 18:34:27   \n",
       "1160  825751377238450178           164369297  2017-01-29 17:04:04   \n",
       "1161  825799292766019585           164369297  2017-01-29 20:14:28   \n",
       "1162  825799094148984834           164369297  2017-01-29 20:13:41   \n",
       "1163  825774711481188352           164369297  2017-01-29 18:36:48   \n",
       "1164  825732902583463936           164369297  2017-01-29 15:50:40   \n",
       "1165  825728324907520001           164369297  2017-01-29 15:32:28   \n",
       "1166  825510364565606401           164369297  2017-01-29 01:06:22   \n",
       "1167  825498115746754561           164369297  2017-01-29 00:17:42   \n",
       "1168  825495311967780865           164369297  2017-01-29 00:06:33   \n",
       "1169  825781814157508608          1072467470  2017-01-29 19:05:01   \n",
       "1170  825772454928842752          1072467470  2017-01-29 18:27:50   \n",
       "1171  825754131864383493          1072467470  2017-01-29 17:15:01   \n",
       "1172  825828584489099264           104198706  2017-01-29 22:10:52   \n",
       "1173  825561879959523329           104198706  2017-01-29 04:31:05   \n",
       "1174  825843236086374400            22523087  2017-01-29 23:09:05   \n",
       "1175  825757776240640001            22523087  2017-01-29 17:29:30   \n",
       "1176  825751005648261121            22523087  2017-01-29 17:02:36   \n",
       "1177  825854237288648704          1140648348  2017-01-29 23:52:48   \n",
       "1178  825787950713729025          1140648348  2017-01-29 19:29:24   \n",
       "1179  825704463289688064           787373558  2017-01-29 13:57:39   \n",
       "1180  825704452300619776           787373558  2017-01-29 13:57:36   \n",
       "1181  825704443794563072           787373558  2017-01-29 13:57:34   \n",
       "1182  825521962164289536           237770636  2017-01-29 01:52:27   \n",
       "\n",
       "                                             tweet_text  \\\n",
       "0     RT @GOPHELP: Betsy DeVos says in letter - \"I a...   \n",
       "1     RT @GOPHELP: Op-ed in @memphisnews:\"DeVos has ...   \n",
       "2     RT @JCColtin: @RepEspaillat inauguration going...   \n",
       "3     RT @NYCMayor: We're the safest big city in Ame...   \n",
       "4     RT @NYCMayor: I'll be joining New Yorkers at B...   \n",
       "5     RT @NYCMayor: There is something more importan...   \n",
       "6     RT @LizRittr: .@BilldeBlasio to @jeffsessions:...   \n",
       "7     Thank you @SenSchumer for your remarks today @...   \n",
       "8     RT @jonorcutt: Great rallying cries from @RepE...   \n",
       "9     RT @altochulo: No justice, no peace! cries @Re...   \n",
       "10    RT @LizRittr: Packed house waiting for the com...   \n",
       "11    RT @KariStrong: Here to support and celebrate ...   \n",
       "12    RT @angelabaggetta: Packed house for the cerem...   \n",
       "13    RT @javierhvaldes: Nuestro hermano @RepEspaill...   \n",
       "14    RT @KelleenLib: Thank you @RepEspaillat 4 bein...   \n",
       "15    RT @ZaharaMZ: .@RepEspaillat calls out NYC wea...   \n",
       "16    RT @rebeccalounyc: @RepYvetteClarke @GregoryMe...   \n",
       "17    RT @marializardo: @RepEspaillat Getting ready ...   \n",
       "18    A Federal Judge Just Issued A Stay Against Don...   \n",
       "19    @RepEspaillat @GregoryMeeks #JFK airport talki...   \n",
       "20    Finding Dory? Maybe @realDonaldTrump should fo...   \n",
       "21    \"History will remember them as cowards.\" Here'...   \n",
       "22    Some good news, but we're not done fighting ye...   \n",
       "23    RT @NancyPelosi: We believe health care is a r...   \n",
       "24    My statement regarding President Trump's execu...   \n",
       "25    Yes!  Even US allies recognize the error in US...   \n",
       "26    The American public has a right to know how @D...   \n",
       "27    Thanks to the ACLU for standing up to the Trum...   \n",
       "28    Turning our backs on refugees\\nbetrays our Ame...   \n",
       "29    RT @tedwheeler: At PDX w/ @SenJeffMerkley and ...   \n",
       "...                                                 ...   \n",
       "1153  @soltanlife on it. Huge, troubling problem. De...   \n",
       "1154  @RossSchulman working on this as we speak. Hug...   \n",
       "1155  RT @MaketheRoadNY: .@NydiaVelazquez @RepJerryN...   \n",
       "1156  RT @HeyItsMurad: JOIN US! They finally let one...   \n",
       "1157  RT @MaketheRoadNY: yesterday @NydiaVelazquez h...   \n",
       "1158  RT @DanaGpdx: Thank you congresswoman @NydiaVe...   \n",
       "1159  Proud to join Brooklyn Chinese-American Associ...   \n",
       "1160  RT @RepJerryNadler: Video from yesterday: @Nyd...   \n",
       "1161  Good rally today in support of refugees! https...   \n",
       "1162  Speaking today at rally against unconstitution...   \n",
       "1163  Speaking to friends at Brooklyn Chinese-Americ...   \n",
       "1164  Fed Courts Must Overturn Trump's EO! Immigrati...   \n",
       "1165  Yesterday's court stay was victory but we must...   \n",
       "1166  RT @HeyItsMurad: Hameed Darwish, #Iraqi #Refug...   \n",
       "1167  RT @JackSmithIV: Inside JFK, fighting for the ...   \n",
       "1168  RT @NydiaVelazquez: At #JFK with @RepJerryNadl...   \n",
       "1169  I'm committed to keeping our river clean and s...   \n",
       "1170  My statement on the #MuslimBan: https://t.co/G...   \n",
       "1171  Interested in what's happening in Washington? ...   \n",
       "1172  Virginians deserve answers about the implement...   \n",
       "1173       Welcome to Virginia! https://t.co/YdsymB5N4h   \n",
       "1174  RT @MayorMeganBarry: Glad to see @SenBobCorker...   \n",
       "1175  RT @ABCPolitics: Former Defense Sec. Bob Gates...   \n",
       "1176  RT @politico: McCain blasts Bannon placement o...   \n",
       "1177  #LillyLedbetter Fair Pay Act signed 8 yrs ago ...   \n",
       "1178  We weaken our nation by turning our back on th...   \n",
       "1179  RT @OversightDems: .@RepCummings: 17 Intel Age...   \n",
       "1180  RT @OversightDems: .@RepCummings: #Trump: if y...   \n",
       "1181  RT @OversightDems: .@RepCummings: This #voterf...   \n",
       "1182  We shouldn't impose a religious test on anyone...   \n",
       "\n",
       "                           hashtag_entities  \n",
       "0                                        {}  \n",
       "1                                        {}  \n",
       "2                                        {}  \n",
       "3                                        {}  \n",
       "4                                        {}  \n",
       "5                             {BatteryPark}  \n",
       "6                                        {}  \n",
       "7                                    {NY13}  \n",
       "8                                        {}  \n",
       "9                             {NoBanNoWall}  \n",
       "10                                       {}  \n",
       "11                                       {}  \n",
       "12                                       {}  \n",
       "13                 {HeretoStay,NoBanNoWall}  \n",
       "14                                       {}  \n",
       "15                            {NoBanNoWall}  \n",
       "16                                       {}  \n",
       "17                                       {}  \n",
       "18                                       {}  \n",
       "19                                    {JFK}  \n",
       "20                      {StandWithRefugees}  \n",
       "21                                       {}  \n",
       "22                      {StandWithRefugees}  \n",
       "23                         {ProtectOurCare}  \n",
       "24                                       {}  \n",
       "25                                       {}  \n",
       "26                        {NoBanOnRefugees}  \n",
       "27                                       {}  \n",
       "28                               {nobanpdx}  \n",
       "29                                       {}  \n",
       "...                                     ...  \n",
       "1153                                     {}  \n",
       "1154                                     {}  \n",
       "1155                            {MuslimBan}  \n",
       "1156         {JFK,T4,NoBanNoWall,MuslimBan}  \n",
       "1157                                     {}  \n",
       "1158            {MuslimBan,RefugeesWelcome}  \n",
       "1159                         {LunarNewYear}  \n",
       "1160                            {MuslimBan}  \n",
       "1161                                     {}  \n",
       "1162                          {NoWallNoBan}  \n",
       "1163                     {yearoftherooster}  \n",
       "1164                                     {}  \n",
       "1165                          {NoWallNoBan}  \n",
       "1166  {Iraqi,Refugee,NoBanNoWall,muslimBan}  \n",
       "1167                                     {}  \n",
       "1168                                  {JFK}  \n",
       "1169                                     {}  \n",
       "1170                            {MuslimBan}  \n",
       "1171                                     {}  \n",
       "1172                            {MuslimBan}  \n",
       "1173                                     {}  \n",
       "1174                                     {}  \n",
       "1175                                     {}  \n",
       "1176                                     {}  \n",
       "1177              {LillyLedbetter,EqualPay}  \n",
       "1178                          {NoBanNoWall}  \n",
       "1179          {RussianHacking,Election2016}  \n",
       "1180                     {Trump,voterfraud}  \n",
       "1181                           {voterfraud}  \n",
       "1182                                     {}  \n",
       "\n",
       "[1183 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a bunch of tweets from senators and their aids on 29 Jan 2017\n",
    "\n",
    "tweets = pd.read_csv('https://raw.githubusercontent.com/data-ppf/data-ppf.github.io/master/dat/Jan_2017_tweets.csv')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique twitter accounts exist in this data set?\n",
    "len(tweets.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets are contained in the data set?\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many years does this data set cover? When does it start and when does it end?\n",
    "\n",
    "earliest_tweet_date = tweets['created_at'].min()\n",
    "print(\"earliest tweet date\", earliest_tweet_date)\n",
    "\n",
    "latest_tweet_date = tweets['created_at'].max()\n",
    "print(\"latest tweet date\", latest_tweet_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we propose to do something very bad with data. We're going to take these tweets and, using PCA, imagine communtities of affective affinity or political belief in two ways: \n",
    "1. by looking at individual hashtag use to make inferences about existing communities; \n",
    "2. by looking at individual language usage to postulate shared affect. \n",
    "\n",
    "It's important to stress that we don't think this is a particularly creditable or justifiable approach, but it does serve to illustrate a fairly predominant trend in which some data is used to posit the existence of different communities and to make claims about individuals.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're going to need some handy functions for stuff we're going to do again and again below. \n",
    "You don't need to pay to much attention to the details below, but you should note that functions are very handy for repetitive tasks. You must run the box below in order to use these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(list_to_search): #uses single_type_count() to count all tokens\n",
    "    unique_words = set(list_to_search)\n",
    "    word_counts = {}\n",
    "    for word in unique_words:\n",
    "        word_counts[word] = single_type_count(word, list_to_search)\n",
    "    return word_counts # dict w/ word counts\n",
    "\n",
    "def single_type_count(token_to_count, list_to_search): #counts up all tokens of a single type\n",
    "    number_of_tokens = 0                            \n",
    "    for token in list_to_search:                   \n",
    "        if token == token_to_count:                 \n",
    "            number_of_tokens += 1                   \n",
    "    return number_of_tokens #returns int\n",
    "\n",
    "def total_number_of_words(dict_of_word_counts): #for use with token_counts\n",
    "    number_of_words = 0\n",
    "    for word in dict_of_word_counts:\n",
    "        number_of_words = number_of_words + dict_of_word_counts[word]\n",
    "    return number_of_words #returns int\n",
    "\n",
    "def total_number_of_words_in_corpus(list_of_total_word_counts):\n",
    "    total_number_of_words = 0\n",
    "    for total in range(0,len(list_of_total_word_counts)):\n",
    "        total_number_of_words = total_number_of_words + list_of_total_word_counts[total]\n",
    "    return total_number_of_words #returns int\n",
    "\n",
    "def get_word_frequencies(dict_of_words_with_counts, total_number_of_words_in_text):\n",
    "    word_freq = {}\n",
    "    for word in dict_of_words_with_counts:\n",
    "        word_freq[word] = dict_of_words_with_counts[word]/total_number_of_words_in_text\n",
    "    return word_freq # dict with word w/ normalized frequencies\n",
    "\n",
    "def word_freq(word_counts, corpus_word_count):\n",
    "## calculate word frequencies for any set of distinct items (e.g., texts, chunks, topic models, etc.)\n",
    "    word_frequencies = []\n",
    "    for text in range(0, len(word_counts)):\n",
    "        word_frequencies.append(get_word_frequencies(word_counts[text],corpus_word_count)) \n",
    "    return word_frequencies\n",
    "\n",
    "def obtain_MFW(word_frequencies, compared_to_which_text, textnames):\n",
    "#USEFUL FOR EXAMINING MOST FREQUENT WORDS IN WORD LIST\n",
    "#Uses Pandas to sort word frequencies and fill empty cells\n",
    "#Returns dataframe \"MFW\"\n",
    "    readable_word_frequencies = pd.DataFrame(word_frequencies).T\n",
    "    #compared_to_which_text = 0 # column identifer for a particular text; full list in \"novelnames\"\n",
    "    MFW = readable_word_frequencies.sort_values([compared_to_which_text], ascending = False)\n",
    "    MFW = MFW.fillna(0) # fill all empty cells with zeros   \n",
    "    print(\"Columns are users/texts\")\n",
    "    print(\"Rows are words/hashtags\")\n",
    "    return MFW\n",
    "\n",
    "def PCAnalysis(word_frequencies, number_of_MFWs_used, corpus_word_count, number_of_components, text_index_to_compare_MFWs):\n",
    "## APPLY PCA TO WORD FREQUENCIES LIST\n",
    "## IF number_of_MFWs_used == 0, USE ALL WORDS IN LIST.\n",
    "    ## prepare word_frequencies for PCA processing\n",
    "    print(\"Corpus Word Count:\" + str(corpus_word_count))\n",
    "    if number_of_MFWs_used == 0:\n",
    "        print(\"\"\"Using corpus word count (\"\"\" + str(corpus_word_count) + \"\"\" words) for PCA in \"\"\" + str(number_of_components) + \"\"\"-dimensions...\"\"\")\n",
    "        number_of_MFWs_used = corpus_word_count\n",
    "        # Note: There's probably a faster way than performing to T operations in the next 3 lines...\n",
    "        dataframe_word_frequencies = pd.DataFrame(word_frequencies).T\n",
    "        MFW = dataframe_word_frequencies.sort_values([text_index_to_compare_MFWs], ascending = False)\n",
    "        MFW_for_PCA = MFW.fillna(0).as_matrix().T #using all MFWs\n",
    "    else:\n",
    "        print(\"Using \" + str(number_of_MFWs_used) + \" words for PCA in \"+ str(number_of_components) + \"-dimensions...\")  #np.nan_to_num(word_frequencies[number_of_MFWs_used:])\n",
    "        #Note: There's probably a faster way than performing to T operaions in the next 3 lines\n",
    "        dataframe_word_frequencies = pd.DataFrame(word_frequencies).T\n",
    "        MFW = dataframe_word_frequencies.sort_values([text_index_to_compare_MFWs], ascending = False)\n",
    "        MFW_for_PCA = MFW.head(number_of_MFWs_used).fillna(0).as_matrix().T #using X of MFWs\n",
    "    ## generate data points of PCA from MFW_for_PCA (this is where PCA is performed)\n",
    "    pca_coordinates, pca_results = perform_PCA(MFW_for_PCA, number_of_components)   \n",
    "    return pca_coordinates, pca_results\n",
    "\n",
    "def perform_PCA(MFWlist_array_for_PCA, num_of_PCs):\n",
    "#perform PCA on using the MFW lists for a set of texts\n",
    "    pca_results = sklearnPCA(n_components = num_of_PCs)\n",
    "    pca_coordinates = pca_results.fit_transform(MFWlist_array_for_PCA) #array of x- & y-coordinates \n",
    "    return pca_coordinates, pca_results\n",
    "\n",
    "def plot_PCA(pca_coordinates, pca_results, colors_for_texts, textnames, plot_size, output_flag, plot_name):\n",
    "## plot PCA graph without any description/labeling of texts; \n",
    "## set output_flag = 1 to produce PDF, 2 to produce PNG, else output = 0   \n",
    "    ## (1) prepare color scheme, data point style & labels ---\n",
    "    for text in range(0, len(pca_coordinates)):        \n",
    "        plt.plot(pca_coordinates[text,0],pca_coordinates[text,1], 'o', markersize=7, color=colors_for_texts[text], alpha=0.5, label=textnames[text])\n",
    "    ## (2) graph display parameters & labels ---\n",
    "    plt.xlabel('PC 1 ('+str(pca_results.explained_variance_ratio_[0]*100)+'%)') #x-axis title\n",
    "    plt.ylabel('PC 2('+str(pca_results.explained_variance_ratio_[1]*100)+'%)') #y-axis title\n",
    "    matplotlib.rcParams['figure.figsize'] = (plot_size, plot_size) #size of graph generated in notebook\n",
    "    ##plt.axis('tight') #OR just fit plot around data automatically; but this usually fits *so* closely that it misses data\n",
    "    plt.title('PCA for ' + str(len(textnames)) + 'users') #title of plot\n",
    "    plt.grid(b=True, which='major', color='gray', linestyle='dotted') # Add gridlines\n",
    "    ## (3) \"print-ready\" plots ---\n",
    "    ## need to produce these files before 'plt.show' since that command erases graph parameters\n",
    "    if output_flag == 1:\n",
    "        plt.savefig(plot_name + '.pdf', dpi=600) #to produce a PDF of plot_PCA\n",
    "    if output_flag == 2:\n",
    "        plt.savefig(plot_name + '.png', dpi=600) #to produce a png of plot\n",
    "    if output_flag != 1 & output_flag != 2 & output_flag != 0:\n",
    "        print(\"Warning: output_flag for PCA plot not set to 0, 1, or 2\")\n",
    "        print(\"see plot_PCA_with_labels function\")\n",
    "    plt.show() ## (4) plot PCA graph to screen ---\n",
    "    return\n",
    "\n",
    "def assign_text_colors_via_word_counts(total_word_counts_nparray, total_word_counts):\n",
    "## assigns colors for texts based on their relative word counts\n",
    "## uses the word_count_binning function\n",
    "    n, bins = word_count_binning(total_word_counts_nparray) #produce binning for all texts or text chunks in corpus\n",
    "    text_binning = [] #identifies the bin in which a text belongs, where the text index is the same used in total_word_counts\n",
    "    for text in range(0, len(total_word_counts_nparray)):\n",
    "        for bin in range (0, len(bins)):\n",
    "            if (total_word_counts[text] >= bins[bin]) & (total_word_counts[text] <= bins[bin+1]):\n",
    "                text_binning.append(bin)\n",
    "                break\n",
    "    colors = generate_spectrum_of_colors(len(n))\n",
    "    colors_for_texts = [] #identifies the color of a text, where the text index is the same used in total_word_counts\n",
    "    for text in range(0, len(text_binning)):\n",
    "        colors_for_texts.append(colors[text_binning[text]])\n",
    "    return colors_for_texts\n",
    "\n",
    "def word_count_binning(total_word_counts_numpy_array):\n",
    "## generates historgram data for all texts or text chunks, based on word count,\n",
    "## but does not output histogram plot; number of bins set to 37\n",
    "    ## (1) generate histogram of data ---------------------------------\n",
    "    n, bin_location_list, patches = plt.hist(total_word_counts_numpy_array, 37, facecolor='red', alpha=0.75) #plt.hist(total_word_counts_array, 2, normed=1, facecolor='red', alpha=0.75)\n",
    "    #TEST: print(\"contents of bins: \" + str(n))\n",
    "    #TEST: print(\"bin location intervals: \" + str(bin_location_list))\n",
    "    plt.close()\n",
    "    return n, bin_location_list \n",
    "\n",
    "def generate_spectrum_of_colors(num_colors): \n",
    "# generates a spectrum of colors--from red to black--for use in matplotlib;\n",
    "    import colorsys\n",
    "    spectrum_of_colors = []\n",
    "    hue = 355/360\n",
    "    for i in np.arange(0., 100., 100. / num_colors):\n",
    "        lightness = i/100\n",
    "        saturation = i/100\n",
    "        spectrum_of_colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
    "    return spectrum_of_colors\n",
    "\n",
    "def plot_PCA_with_labels(pca_coordinates, pca_results, textnames, colors_for_texts, plot_size, output_flag, plot_name):\n",
    "## plot PCA graph using \"arrow\" labels; set output_flag = 1 to produce PDF, 2 to produce PNG, else output = 0\n",
    "## WARNING: Using some styles for plt.style.use() will *break* arrow labels. For instance,\n",
    "## plt.style.use('ggplot') breaks arrows. Accordingly, this function sets plt.style.use('classic')\n",
    "    ## (0) invoke plt.style that works with arrow labels\n",
    "    plt.style.use('seaborn-whitegrid') #works w/ 'seaborn-ticks', 'seaborn-white', classic'\n",
    "     ## (1) prepare color scheme, data point style & labels ---\n",
    "    for text in range(0, len(pca_coordinates)):\n",
    "        plt.plot(pca_coordinates[text,0],pca_coordinates[text,1], 'o', markersize=7, color=colors_for_texts[text], alpha=0.8, label=textnames[text])\n",
    "    ## (2) graph display parameters & labels ---\n",
    "    plt.xlabel('PC 1 ('+str(pca_results.explained_variance_ratio_[0]*100)+'%)') #x-axis title\n",
    "    plt.ylabel('PC 2 ('+str(pca_results.explained_variance_ratio_[1]*100)+'%)') #y-axis title\n",
    "    matplotlib.rcParams['figure.figsize'] = (plot_size, plot_size) #size of graph generated in notebook\n",
    "    plt.title('PCA for ' + str(len(textnames)) + ' novels') #title of plot\n",
    "    ax = plt.subplot(111) #used in making legend \n",
    "    plt.grid(b=True, which='major', color='gray', linestyle='dotted') #grid lines\n",
    "    ## (3)generate arrow labels ---\n",
    "    for text in range(0, len(textnames)):\n",
    "        ax.annotate(textnames[text], xy=(pca_coordinates[text,0], pca_coordinates[text,1]), xycoords='data', xytext=(-30, -30), textcoords='offset points', arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))\n",
    "    ## (4) \"print-ready\" plots ---\n",
    "    ## need to produce these files before 'plt.show' since that command erases graph parameters\n",
    "    if output_flag == 1:\n",
    "        plt.savefig(plot_name + '.pdf', dpi=600) #to produce a PDF of plot_PCA\n",
    "    if output_flag == 2:\n",
    "        plt.savefig(plot_name + '.png', dpi=600) #to produce a png of plot\n",
    "    if output_flag != 1 & output_flag != 2 & output_flag != 0:\n",
    "        print(\"Warning: output_flag for PCA plot not set to 0, 1, or 2\")\n",
    "        print(\"see plot_PCA_with_labels function\")\n",
    "    ## (5) plot PCA graph to screen ---\n",
    "    plt.show() \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2, Section 1: Using hashtags to invent communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all unique twitter users in corpus\n",
    "users = list(set(tweets.user_id.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all hashtags for each user; counts of all hashtags for all users; \n",
    "# total number of hashtags for all users; get all total number of all hashtags;\n",
    "# and hashtag frequencies in corpus\n",
    "\n",
    "user_hashtags = []                 # list of lists of user tag tokens\n",
    "user_hashtag_counts = []           # list of dicts of user tag types\n",
    "user_total_number_of_hashtags = [] # list of number of tag tokens used by each user\n",
    "\n",
    "for ID in range(0, len(users)):  \n",
    "    temp_hashtag = []\n",
    "    hashtag = tweets[tweets[\"user_id\"]==users[ID]][\"hashtag_entities\"].tolist() #list of all hashtags for one user ID\n",
    "    for tag in range(0,len(hashtag)):  # for all hashtags for one user ID\n",
    "        temp_hashtag.extend(re.compile('\\w+').findall(hashtag[tag].lower())) # pull hashtag from one tweet and add to list\n",
    "    user_hashtags.append(temp_hashtag)\n",
    "    user_hashtag_counts.append(count_words(user_hashtags[ID])) \n",
    "    user_total_number_of_hashtags.append(total_number_of_words(user_hashtag_counts[ID])) \n",
    "\n",
    "# number of all tag tokens in tweet corpus\n",
    "corpus_hashtag_count = total_number_of_words_in_corpus(user_total_number_of_hashtags)\n",
    "\n",
    "# calculate frequencies for each hashtag in corpus\n",
    "hashtag_frequencies = word_freq(user_hashtag_counts, corpus_hashtag_count)\n",
    "\n",
    "# TESTS\n",
    "#print(user_hashtags)\n",
    "#print(user_hashtag_counts)    \n",
    "#print(user_total_number_of_hashtags)\n",
    "#print(corpus_hashtag_count)\n",
    "#print(hashtag_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While the following code isn't necessary to use PCA, it shows you the top 10 most used hashtags in the twitter corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect most frequent words (MFWs) ------------------\n",
    "text_index_to_compare_MFWs = 0 # column identifer for a particular text; full list in \"textnames\"\n",
    "most_frequent_hashtags = obtain_MFW(hashtag_frequencies, text_index_to_compare_MFWs, str(users))\n",
    "\n",
    "\n",
    "# Columns are individual texts (i.e., novels in this case), rows are words\n",
    "most_frequent_hashtags.head(10) #list first 10 words for all users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets actually do PCA on the hashtags for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform PCA for users using hashtags \n",
    "\n",
    "text_index_to_compare_MFTs = 0 # column identifer for a particular text; full list in \"textnames\"\n",
    "number_of_components = 2 # how many dimensions for PCA\n",
    "number_of_MFTs_used = 0 #number of MFWs used to plot PCA graphs # if set to 0, will use all words\n",
    "pca_coordinates, pca_results = PCAnalysis(hashtag_frequencies, number_of_MFTs_used, corpus_hashtag_count, number_of_components, text_index_to_compare_MFTs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now lets plot our PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate color spectrum based on tag count of texts ---------\n",
    "total_tag_counts_nparray = np.array(user_total_number_of_hashtags) # convert word count list into a numpy array\n",
    "colors_for_tags = assign_text_colors_via_word_counts(total_tag_counts_nparray, user_total_number_of_hashtags)\n",
    "\n",
    "## plot PCA data with labels and arrows\n",
    "size_of_plot = 10\n",
    "name_of_file = \"pca-2017-01-29_hashtags_with_labels\"\n",
    "output_file = 1 # for PDF, output_flag = 1; for PNG, output_flag = 2; else output_flag = 0\n",
    "plot_PCA_with_labels(pca_coordinates, pca_results, users, colors_for_tags, size_of_plot, output_file, name_of_file) # last two inputs are plot_size and output flag \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to generate PCA plot without labels, uncomment the single hashes below and run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May need to run this code block twice to get it to display properly!\n",
    "\n",
    "## generate color spectrum based on tag count of texts ----------\n",
    "#total_tag_counts_nparray = np.array(user_total_number_of_hashtags) # convert word count list into a numpy array\n",
    "#colors_for_tags = assign_text_colors_via_word_counts(total_tag_counts_nparray, user_total_number_of_hashtags)\n",
    "\n",
    "##plot PCA without legend or tables -------\n",
    "#size_of_plot = 12\n",
    "#output_file = 0 # for PDF, output_flag = 1; for PNG, output_flag = 2; else output_flag = 0\n",
    "#name_of_file = \"pca-2017-01-29_hashtags\"\n",
    "#plot_PCA(pca_coordinates, pca_results, colors_for_tags, users, size_of_plot, output_file, name_of_file)  # last three inputs are plot_size, output flag, graph name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2, Section 2: Using words to invent communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words for each user; counts of all words for all users; \n",
    "# total number of words for all users; get all total number of all words;\n",
    "# and words frequencies in corpus\n",
    "\n",
    "user_words = []                 # list of lists of user word tokens\n",
    "user_word_counts = []           # list of dicts of user word types\n",
    "user_total_number_of_words = [] # list of number of words used by each user\n",
    "\n",
    "for ID in range(0, len(users)):  \n",
    "    temp_words = []\n",
    "    words = tweets[tweets[\"user_id\"]==users[ID]][\"tweet_text\"].tolist() #list of all hashtags for one user ID\n",
    "    for tweet_wordlist in range(0,len(words)):  # for all hashtags for one user ID\n",
    "        temp_words.extend(re.compile('\\w+').findall(words[tweet_wordlist].lower())) # pull hashtag from one tweet and add to list\n",
    "    user_words.append(temp_words)\n",
    "    user_word_counts.append(count_words(user_words[ID])) \n",
    "    user_total_number_of_words.append(total_number_of_words(user_word_counts[ID])) \n",
    "\n",
    "# number of all tag tokens in tweet corpus\n",
    "corpus_word_count = total_number_of_words_in_corpus(user_total_number_of_words)\n",
    "\n",
    "# calculate frequencies for each hashtag in corpus\n",
    "word_frequencies = word_freq(user_word_counts, corpus_word_count)\n",
    "\n",
    "# TESTS\n",
    "#print(user_words)\n",
    "#print(user_word_counts)    \n",
    "#print(user_total_number_of_words)\n",
    "#print(corpus_word_count)\n",
    "#print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect most frequent words (MFWs) ------------------\n",
    "text_index_to_compare_MFWs = 0 # column identifer for a particular text; full list in \"textnames\"\n",
    "MFWs= obtain_MFW(word_frequencies, text_index_to_compare_MFWs, str(users))\n",
    "\n",
    "\n",
    "# Columns are individual texts (i.e., novels in this case), rows are words\n",
    "MFWs.head(10) #list first 10 words for all users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform PCA for users using hashtags \n",
    "\n",
    "text_index_to_compare_MFWs = 0 # column identifer for a particular text; full list in \"textnames\"\n",
    "number_of_components = 2 # how many dimensions for PCA\n",
    "number_of_MFWs_used = 0 #number of MFWs used to plot PCA graphs # if set to 0, will use all words\n",
    "pca_coordinates, pca_results = PCAnalysis(word_frequencies, number_of_MFWs_used, corpus_word_count, number_of_components, text_index_to_compare_MFWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate color spectrum based on tag count of texts ---------\n",
    "total_word_counts_nparray = np.array(user_total_number_of_words) # convert word count list into a numpy array\n",
    "colors_for_words = assign_text_colors_via_word_counts(total_word_counts_nparray, user_total_number_of_words)\n",
    "\n",
    "## plot PCA data with labels and arrows\n",
    "size_of_plot = 10\n",
    "name_of_file = \"pca-2017-01-29_words_with_labels\"\n",
    "output_file = 1 # for PDF, output_flag = 1; for PNG, output_flag = 2; else output_flag = 0\n",
    "plot_PCA_with_labels(pca_coordinates, pca_results, users, colors_for_tags, size_of_plot, output_file, name_of_file) # last two inputs are plot_size and output flag \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May need to run this code block twice to get it to display properly!\n",
    "\n",
    "## generate color spectrum based on tag count of texts ----------\n",
    "total_word_counts_nparray = np.array(user_total_number_of_words) # convert word count list into a numpy array\n",
    "colors_for_words = assign_text_colors_via_word_counts(total_word_counts_nparray, user_total_number_of_words)\n",
    "\n",
    "##plot PCA without legend or tables -------\n",
    "size_of_plot = 12\n",
    "output_file = 0 # for PDF, output_flag = 1; for PNG, output_flag = 2; else output_flag = 0\n",
    "name_of_file = \"pca-2017-01-29_words\"\n",
    "plot_PCA(pca_coordinates, pca_results, colors_for_tags, users, size_of_plot, output_file, name_of_file)  # last three inputs are plot_size, output flag, graph name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
