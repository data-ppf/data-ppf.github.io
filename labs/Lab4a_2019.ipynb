{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Past, Present, Future |  Lab 4  |  2/14/2019\n",
    "\n",
    "\n",
    "## describing and predicting: Galton, regression, inventing error, survival curves, smoothing, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galton and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galton's analysis \"gives the numerical value of the regression towards mediocrity in the case of human stature, as from 1 to 2/3 with unexpected coherence and precision [see Plate IX, fig. (a)]\"\n",
    "\n",
    "![Plate_9](https://www.researchgate.net/profile/Yeming_Ma2/publication/280970132/figure/fig1/AS:284517131669510@1444845578444/Rate-of-regression-in-hereditary-stature-Galton-1886-Plate-IX-fig-a-The-short_Q320.jpg)\n",
    "\n",
    "The paper can be found at:\n",
    "\n",
    "http://www.stat.ucla.edu/~nchristo/statistics100C/history_regression.pdf\n",
    "\n",
    "Download it and follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galton's data \n",
    "\n",
    "![galton_notebook](http://www.medicine.mcgill.ca/epidemiology/hanley/galton/notebook/images/1_page_1.jpg)\n",
    "(h/t http://www.medicine.mcgill.ca/epidemiology/hanley/galton/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some of Galton's data from his study exploring the relationship between the heights of adult children and the heights of their parents. The data includes the following fields:\n",
    "\n",
    "\n",
    "    Family: The family that the child belongs to, labeled from 1 to 204 and 136A\n",
    "    Father: The father's height, in inches\n",
    "    Mother: The mother's height, in inches\n",
    "    Gender: The gender of the child, male (M) or female (F)\n",
    "    Height: The height of the child, in inches\n",
    "    Kids: The number of kids in the family of the child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library, denoting library as \"pd\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights=pd.read_csv(\"http://www.randomservices.org/random/data/Galton.txt\", sep=\"\\t\") # Note \"\\t\" = tab separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights[\"Height\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want only the people identified as male. We can use something called boolean indexing to pick out just the males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_male=heights[heights[\"Gender\"]==\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_male.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use some of our favorite tools in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights[\"Height\"].hist()\n",
    "heights_male[\"Height\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(heights[\"Father\"],heights[\"Height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(heights_male[\"Father\"],heights_male[\"Height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galton wanted to use the data on women. What would he need to do?\n",
    "\n",
    ">In everycase I transmuted the female statures to their corresponding male equivalents and used them in their transmuted form, so that no objection grounded on the sexual difference of stature need be raised when I speak of averages. The factor I used was 1-08, which is equivalent to adding a little less than one-twelfth to each female height.\n",
    "\n",
    "Hello, subjective design choice!\n",
    "\n",
    "So how do we do this?\n",
    "\n",
    "In python, as easy to multiple every value in a column by a given amount as it is to multiple one value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick out all the women using boolean indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE THE DOUBLE EQUALS (==) which says \"python ARE these equal\" NOT \"python SET these equal\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights[heights[\"Gender\"]==\"F\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pick out only the heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights[heights[\"Gender\"]==\"F\"][\"Height\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's multiply all the women's heights by 1.09 as Galton tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights[heights[\"Gender\"]==\"F\"][\"Height\"]*1.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually Galton had computer trouble:\n",
    ">owing to a mistaken direction, the computer to whom I first entrusted the figures used a  somewhat different factor, yet the result came out closely the same.\n",
    "\n",
    "Been there, bro. Been there.\n",
    "\n",
    "What does \"computer\" mean here?\n",
    "\n",
    "And now put it all together by replacing the old values with the \"transmuted\" ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_transmuted=heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_transmuted.loc[heights[\"Gender\"]==\"F\",\"Height\"]=heights[heights[\"Gender\"]==\"F\"][\"Height\"]*1.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights_transmuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Galton decides to combine the heights of the father and mother, transmuted by 1.08, to create what he calls the *midparent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midheights=(heights_transmuted[\"Father\"]+1.08*heights_transmuted[\"Mother\"])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, it's almost too easy to do linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermission ------\n",
    "\n",
    "## Recall the \"double role\" of statistics in Politics:\n",
    "1. <b>construction of statistical entities</b>: \"stable objects\" can be measured and used as forms of evidence and certainty (e.g., the GDP, unemployment, life expectation, citation indexs, etc.)\n",
    "2. <b>explication and analysis of relationships between entities</b>: what are the relationships between objects and how does changing one influence others? \n",
    "\n",
    "<small>(See, for instance, Desrosieres, _The Politics of Large Numbers_ (1998), 61.)</small>\n",
    "\n",
    "#### To better understand how statistical entities' \"double role\" in being used to make an argument, let's look at linear regression. But we first need to understand the innovations that made it thinkable to model error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventing Error -----\n",
    "\n",
    "Let's say you want to determine the position of a star in the sky. You have bunch of observations of the sky you produced here just outside of New York. You also have a bunch of observations taken in Hawaii. Today we'd just combine the observations, assuming that the distribution of the error is a gaussian. Imagine we are measuring the position of a star *in only 1-dimension*, i.e., we are measuring a star only along the x-axis.\n",
    "![star_obs](Star.obs.jpeg)\n",
    "<small>(Note that this is an example of an \"objective mean\" we discussed in class.)</small>\n",
    "\n",
    "\n",
    "#### However, it requires an argument to assume error would be distributed in this way! \n",
    "It wasn't obvious that error would be normally distributed at the start of the 19th century! In the 18th century they tried to deal with questions of modeling error either (1) by averaging observations (as we did above with smoothing) to reduce the number of equations or (2) to minimize the sum of the absolute values of the residuals. <b> In 1805 Legendre gave the \"method of least squares\"</b> in which we have the following situation:\n",
    "![Residuals!](Residuals.jpeg)\n",
    "where each residual, $r_i$, can be written as\n",
    "\n",
    "$ r_i = y_i - y(x_i, \\beta_i)$\n",
    "\n",
    "which is the distance between the $i^{th}$ observation $y_i$. The \"best fit line\" is $y = \\beta_0 + \\beta_1(x)$, denoted in blue above, and satisfies the following \"least squares\" condition:\n",
    "\n",
    "$min \\sum{r_i} = min \\sum_{i = 1}^{i}{(y_i-y(x_i,\\beta_i))^2}$\n",
    "\n",
    "--that is, the line minimizes the square of the residuals.  \n",
    "\n",
    "The least squares method was the solution to an empirical problem <b>as long as the error could be assumed to be randomly produced</b>, but in 1810 Laplace realized that any distribution of errors (i.e., residuals) would produce a gaussian. What did he \"discover\" to justify this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Central Limit Theorem:</b> \n",
    "0. Start with *any* distribution, known or unknown. \n",
    "1. Take $N$ samples of $X$ observations.\n",
    "2. Take the mean of $X$ observations for each sample.\n",
    "3. Plot the means of the samples.\n",
    "4. The histogram of the sample of the means will <b>tend toward a gaussian as $N \\rightarrow \\infty $\n",
    "\n",
    "...*that is, regardless of the distribution of your residuals, the means of $N$ samples of $X$ observations will always produce a gaussian distribution when one takes enough samples!*\n",
    "\n",
    "Lets verify this for ourselves! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, lets use $\\alpha = \\beta = .5$ for our beta distribution (i.e., the blue curve above) since this looks the least like a gaussian. \n",
    "\n",
    "Now lets examine a few plots so see how producing a historgram of the mean of more and more samples eventually converges to a gaussian distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took nearly a century to build off of the Gauss-Laplace synthesis, largely because of the interpretative challenge of equating objective and subjective means as interchangable  (something that happened with Pearson, Galton, and others at then end of the 19th century)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares and Regression\n",
    "Let's take a look at our residuals graph from earlier. \n",
    "![Residuals!](Residuals.jpeg)\n",
    "Today we tend to see a plot like this in the context of linear regression. The task of linear regression is just to find the blue line for a particular set of data, via the method of least squares discussed above. Thanks to python, it's very, very easy to do simple linear regression using least squares.  \n",
    "\n",
    "Linear Regression IN GENERAL:\n",
    "\n",
    "$y = \\beta_n x_n + ... + \\beta_1 x_1 + \\mu_0$\n",
    "\n",
    "Linear Regression FOR JUST ONE VARIABLE:\n",
    "\n",
    "$y = \\beta_1 x_1 + \\mu$ \n",
    "\n",
    "where $\\beta_1$ is the slope, $x$ are the observations, and $\\mu$ is the y-intercept.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Galton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galton was a bit fuzzy on much of this math. Let's do some linear regressions on his height data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#first we say, hey! we want to do linear regression\n",
    "skl_lm = linear_model.LinearRegression()\n",
    "#it's like cool...but on what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that scikit-learn requires an input-matrix of a particular shape...\n",
    "#here we just transform our data into the format it requires\n",
    "x=heights_transmuted[\"Father\"]\n",
    "y=heights_transmuted[\"Height\"]\n",
    "x=np.array(x.values.tolist())\n",
    "y=np.array(y.values.tolist())\n",
    "x = x.reshape(len(x), 1)\n",
    "y = y.reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model\n",
    "skl_lm.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've run the model, we can ask for the coefficient $\\beta_1$ of the equation\n",
    "\n",
    "\n",
    "$y = \\beta_1 x_1 + \\mu$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, python what's $\\beta_1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1=skl_lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fit line\n",
    "plt.scatter(x, y,  color='black')\n",
    "plt.plot(x, skl_lm.predict(x), color='blue', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now it's your turn!* \n",
    "\n",
    "can you the regression for \n",
    "\n",
    "1. everybody and his/her mother?\n",
    "2. males and fathers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## death curves since Halley\n",
    "\n",
    "(Yes, Halley of the comet.) Quantifying morality goes back to the 17th century with the work of Halley:\n",
    "![Halley table](https://understandinguncertainty.org/sites/understandinguncertainty.org/files/halley-life-table.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to look at how they were put to work to make money, contempoary with our readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Survivor curves and making smoothing \"real\" at the dawn of the 20th Century\n",
    "\n",
    "In 1905 a New York Lawyer named Charles Evans Hughes put American corporations on trial: namely, the Big 5 insurances companies at the time had steadily raised salaries of high-ranking employees even as policyholder dividends continued to fall [1]. Hughes was tasked with running a state investigation colloquially known as the \"Armstrong investigation\" to find out why, and began interviewing some of the very people whose high salaries were being questioned. Particularly sensational to the reading public following this \"investigation\" was the discovery that insurance actuaries did not use the empirically observed number of deaths, expense account balances, or exact annual interests on investiments [2]. Instead insurance companies <b>calculated \"smoothed curves\" from data, arguing that such smoothed curves more accurately  described reality</b>. Here early 20th C actuaries were borrowing a page from early 19th C astronomers. For astronomers, the mean position of, say, the position of a star in the sky became its *real* position, even if the mean matched no particular observation, as long as the errors of observation could be assumed to be random. This was a kind of smoothing via least squares [3]. Perhaps this makes sense when observing an object in the sky, but did it make sense to treat errors as deviations: to smooth life expectancies, expense sheets, annual investiment interest and treat these as the \"correct\" values to calculate risk, make business decisions, and mail annuity payments? Many in life insurance thought it did. In fact, some in insurance saw smoothing as a way not merely to describe society and assign individual risk, but even as a means of improving society by attempting to decrease individual statistical risks, as can be seen in this plot in a book co-authored by Louis Dublin, a vice-president of MET Life Insurance Company in 1931 [4]:\n",
    "\n",
    "![new figure](fig/pg.195.png)\n",
    "\n",
    "<small>\n",
    "[1] For a discussion of the Armstrong Investigation, smoothing, and other details discussed above, see chapter 4 in Dan Bouk's _How Our Days Became Numbered_ (2015).\n",
    "[2] Bouk, 93. \n",
    "[3] Desrosi√®res, _The Politics of Large Numbers_ (2002), chapter 3.\n",
    "[4] Louis Dublin and Alfred Lotka, _Length of life; a study of the life table_, 191-195. \n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> In this lab we're going to try out several forms of smoothing in this lab.</b> To begin, let's construct a life expectancy plot like that above. We'll start with three lists which contains the age of death for three different careers: poets, singers, and writers. (Importantly we haven't told you where we got this data so you should immediately be suspicious of this data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet_death = [24,25,26,28,29,29,29,30,32,33,36,36,37,37,37,37,38,38,39,39,39,39,40,41,42,42,\n",
    "                 42,42,43,44,44,45,45,45,46,46,46,46,46,47,47,47,48,48,49,49,49,50,50,51,51,51,\n",
    "                 52,52,52,52,52,53,53,53,54,54,55,55,55,55,56,56,56,56,56,57,57,57,58,58,58,58,\n",
    "                 59,59,59,59,59,60,60,61,61,62,62,62,62,63,64,64,65,65,65,65,66,66,66,66,67,67,\n",
    "                 67,67,67,68,68,68,68,68,68,68,68,69,69,69,69,69,69,69,69,70,70,70,70,70,71,71,\n",
    "                 71,71,71,71,71,71,71,72,72,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,75,\n",
    "                 75,75,75,75,75,76,76,76,77,77,77,77,78,78,78,78,79,79,79,79,79,79,79,80,80,80,\n",
    "                 80,81,81,81,81,81,81,81,81,81,82,82,83,83,83,83,83,83,83,83,83,84,84,84,84,85,\n",
    "                 85,85,85,85,85,85,86,86,86,86,87,87,88,88,88,89,89,89,89,89,89,89,90,90,90,91,\n",
    "                 91,91,93,93,94,101,107]\n",
    "\n",
    "singer_death = [21,27,30,33,40,42,43,44,45,46,47,52,53,53,53,54,55,57,60,61,65,69,69,70,72,72,\n",
    "                75,77,78,78,79,80,81,82,83,84,85,88,89,93,96]\n",
    "\n",
    "writer_death = [29,30,34,38,39,40,40,42,43,44,48,48,50,51,51,52,52,52,54,55,55,55,56,56,57,57,\n",
    "                57,57,57,58,60,60,60,60,60,61,61,61,62,62,63,63,63,64,64,64,65,65,65,65,65,65,\n",
    "                66,66,67,67,67,67,68,68,68,69,69,69,70,70,71,71,71,71,71,71,71,72,72,72,73,73,\n",
    "                74,74,74,75,75,75,75,76,76,76,76,77,77,77,77,77,78,78,78,79,79,79,80,80,80,80,\n",
    "                81,81,81,81,82,82,82,82,83,83,84,84,85,86,87,88,88,88,89,89,89,90,90,95,95,96,\n",
    "                50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to import a few libraries! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library, denoting library as \"pd\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# import a bit of ipython \"magic\" to get graphs to display in jupyter notebook\n",
    "%pylab inline           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ingest the data into pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet_lifespans=pd.DataFrame(poet_death)\n",
    "singer_lifespans=pd.DataFrame(singer_death)\n",
    "writer_lifespans=pd.DataFrame(writer_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the longest living writer, singer, and poet in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"oldest singer: \" + str(singer_lifespans.max().item()))\n",
    "print(\"oldest poet: \" + str(poet_lifespans.max().item()))\n",
    "print(\"oldest writer: \"+ str(writer_lifespans.max().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening above? Let's consider just the singers:\n",
    "1. First we select the max value: \n",
    "<code>singer_lifespans.max()</code>\n",
    "2. Then we pull that value out of the data set as a number via the .item(): \n",
    "<code>singer_lifespans.max().item()</code>\n",
    "3. Finally, we wrap the number in a string so to print out the number: \n",
    "<code>str(singer_lifespans.max().item())</code>\n",
    "\n",
    "\n",
    "Likewise, let's now calculate the life expectancy for these three different professions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"singer life expectancy: \" + str(singer_lifespans.mean().item()))\n",
    "print(\"poet life expectancy: \" + str(poet_lifespans.mean().item()))\n",
    "print(\"writer life expectancy: \" + str(writer_lifespans.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve that we saw Dublin and Lotka provide above is often called a survivor curve. Let's compare the survivor curves for poets, singers, and writers. Let's start with poets. \n",
    "\n",
    "What we need to generate a list called \"poet_survivors_percentages\" in which each element of the list is the percentage of the survivors for a given year:\n",
    "\n",
    "![poet-survivor-list](fig/poet-survivor-list.jpeg)\n",
    "\n",
    "\n",
    "We're going to use a for loop to generate this list. Here is what we need to do: \n",
    "\n",
    "For age X, running from age 0 to the oldest poet age: \n",
    "1. Grab poets alive at age X, i.e., poet_lifespans[poet_lifespans >= age]. \n",
    "    All people younger than age X will return NaN, i.e., NaN = \"not a number\". \n",
    "2. Drop all poets who died at younger age (by dropping all NaN elements), i.e., .dropna().\n",
    "3. Now count up total number of remaining elements, i.e., .size. Note that this is the number of people still alive. \n",
    "4. Divide the number of living poets at age X by total number of poets in sample to get fraction of poets alive at age X. \n",
    "5. Add this fraction alive at age X to the list \"poet_survivors_percentages\".\n",
    "6. Repeat steps 1 - 5 until age X > oldest poet age. \n",
    "\n",
    "We need to do the above steps for each profession too. \n",
    "\n",
    "Earlier we noticed that the oldest poet lived to 107 years old so we'll want to be sure our x-axis goes to at least 107. Note that the \"current_age\" list we generate below contains the specific age for each element of the poet_survivor_precentages list. This will be used as x-axis positions for each (y-axis) life expectancy value for a given age and profession. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate survivorship curves for singers, writers, and poets\n",
    "\n",
    "poet_survivors = []\n",
    "poet_survivor_percentages = []\n",
    "singer_survivors = []\n",
    "singer_survivor_percentages = []\n",
    "writer_survivors = []\n",
    "writer_survivor_percentages = []\n",
    "\n",
    "current_age = []\n",
    "total_poets = poet_lifespans.size      # total number of poets in sample\n",
    "total_singers = singer_lifespans.size  # total number of singers in sample \n",
    "total_writers = writer_lifespans.size  # total number of writers in sample\n",
    "\n",
    "for age in range(0, poet_lifespans.max().item()):\n",
    "    poet_survivors = poet_survivors + [poet_lifespans[poet_lifespans >= age].dropna().size]\n",
    "    poet_survivor_percentages = poet_survivor_percentages + [poet_survivors[age]/total_poets]\n",
    "    \n",
    "    singer_survivors = singer_survivors + [singer_lifespans[singer_lifespans >= age].dropna().size]\n",
    "    singer_survivor_percentages = singer_survivor_percentages + [singer_survivors[age]/total_singers]\n",
    "    \n",
    "    writer_survivors = writer_survivors + [writer_lifespans[writer_lifespans >= age].dropna().size]\n",
    "    writer_survivor_percentages = writer_survivor_percentages + [writer_survivors[age]/total_writers]\n",
    "    \n",
    "    current_age = current_age + [age]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put all these survivor percentages into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivorship_percentages = pd.DataFrame(\n",
    "                            {\"age\": current_age,\n",
    "                            \"poet survivor percentage\": poet_survivor_percentages,\n",
    "                            \"singer survivor percentage\": singer_survivor_percentages, \n",
    "                            \"writer survivor percentage\": writer_survivor_percentages \n",
    "                            })\n",
    "\n",
    "survivorship_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's graph these survivor curves for each profession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivorship_percentages.plot(x=\"age\", y=[\"poet survivor percentage\", \"singer survivor percentage\", \n",
    "                              \"writer survivor percentage\"], figsize=[13,6],\n",
    "                              title=\"Survivorship for 3 different professions\", grid =True)\n",
    "plt.ylabel('percentage still living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the same survivor graph as Dublin and Lotka provided above. <b>What information does this graph propose to present?</b> If we choose to use the above graph to predict the future rather than to merely describe the past, which profession (or professions) should you choose to maximize your life (assuming your only 3 options are between being a singer, poet, or writer)? What are the (many) problems with trying to predict the future with this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now let's return to survivor curves and smoothing. </b> Recall the following plot from above, and pay special attention to what's happening to the singers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivorship_percentages.plot(x=\"age\", y=[\"poet survivor percentage\", \"singer survivor percentage\", \n",
    "                              \"writer survivor percentage\"], figsize=[13,6],\n",
    "                              title=\"Survivorship for 3 different professions\", grid =True)\n",
    "plt.ylabel('percentage still living')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how from approximately ages 22 to 75, the graph suggests you're more likely to die if you're a singer, but if you make it to your early 90s you're more likely to be alive than either poets or writers. What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT\n",
    "print(poet_lifespans.size)\n",
    "print(singer_lifespans.size)\n",
    "print(writer_lifespans.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER HINT\n",
    "poet_lifespans.hist(bins=poet_lifespans.max().item())\n",
    "singer_lifespans.hist(bins=singer_lifespans.max().item())\n",
    "writer_lifespans.hist(bins=writer_lifespans.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When actuaries at the outset of the 20th C turned to smoothing, they did so in part out of a practical necessity of having highly variable data. Yet this was not the only reason. Just as bankers in the 1830s turned to smoothing to hide the extreme daily and weekly variation so as to prevent a bank run, so too did actuaries make use of smoothing to emphasize regularity rather than chance [4]. \n",
    "\n",
    "\n",
    "<small>[4] Bouk, _How Our Days Became Numbered_ (2015), 94-95, 100-101.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our hand at smoothing our survival curves. First we have to average together sets of lifespans for each profession. Let's average 3 deaths at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOOTH POET DEATHS\n",
    "smoothed_poet_death = []\n",
    "for set_of_3 in range(0, len(poet_death), 3): \n",
    "    if set_of_3 not in {237, 240}:\n",
    "        smoothed_poet_death = smoothed_poet_death + [(poet_death[set_of_3]\n",
    "                                                      + poet_death[set_of_3+1]\n",
    "                                                      + poet_death[set_of_3+2]\n",
    "                                                     )/3]  \n",
    "    elif set_of_3 == 237:\n",
    "        # Here we avg 4 instead of 3 death ages\n",
    "        smoothed_poet_death = smoothed_poet_death + [(poet_death[set_of_3]\n",
    "                                                      + poet_death[set_of_3+1]\n",
    "                                                      + poet_death[set_of_3+2]\n",
    "                                                      + poet_death[set_of_3+3]\n",
    "                                                     )/4]  \n",
    "\n",
    "# SMOOTH SINGER DEATHS\n",
    "smoothed_singer_death = []\n",
    "for set_of_3 in range(0, len(singer_death), 3):\n",
    "    if set_of_3 not in {39}:\n",
    "        smoothed_singer_death = smoothed_singer_death + [(singer_death[set_of_3]\n",
    "                                                          + singer_death[set_of_3+1]\n",
    "                                                          + singer_death[set_of_3+2]\n",
    "                                                         )/3] \n",
    "    elif set_of_3 == 39:\n",
    "        # Here we avg 2 instead of 3 death ages\n",
    "        smoothed_singer_death = smoothed_singer_death + [(singer_death[set_of_3]\n",
    "                                                          + singer_death[set_of_3+1]\n",
    "                                                         )/2] \n",
    "\n",
    "        \n",
    "        \n",
    "# SMOOTH WRITER DEATHS\n",
    "smoothed_writer_death = []\n",
    "for set_of_3 in range(0, len(writer_death), 3): \n",
    "    if set_of_3 not in {129}:\n",
    "        smoothed_writer_death = smoothed_writer_death + [(writer_death[set_of_3]\n",
    "                                                          + writer_death[set_of_3+1]\n",
    "                                                          + writer_death[set_of_3+2]\n",
    "                                                         )/3]  \n",
    "    elif set_of_3 == 126:\n",
    "        # Here we avg 5 instead of 3 death ages\n",
    "        smoothed_writer_death = smoothed_writer_death + [(writer_death[set_of_3]\n",
    "                                                          +writer_death[set_of_3+1]\n",
    "                                                          +writer_death[set_of_3+2]\n",
    "                                                          +writer_death[set_of_3+3]\n",
    "                                                          +writer_death[set_of_3+4]\n",
    "                                                         )/5]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the smoothed data, we how does our calculated life expectancy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_poet_lifespans=pd.DataFrame(smoothed_poet_death)\n",
    "s_singer_lifespans=pd.DataFrame(smoothed_singer_death)\n",
    "s_writer_lifespans=pd.DataFrame(smoothed_writer_death)\n",
    "\n",
    "print(\"singer life expectancy: \" + str(singer_lifespans.mean().item()))\n",
    "print(\"poet life expectancy: \" + str(poet_lifespans.mean().item()))\n",
    "print(\"writer life expectancy: \" + str(writer_lifespans.mean().item()))\n",
    "\n",
    "print(\"smoothed singer life expectancy: \" + str(s_singer_lifespans.mean().item()))\n",
    "print(\"smoothed poet life expectancy: \" + str(s_poet_lifespans.mean().item()))\n",
    "print(\"smoothed writer life expectancy: \" + str(s_writer_lifespans.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So smoothing doesn't make too big of difference with our data averages. Let's compare our smoothed survivor curves to our original survivor curves. We do the same thing as we did before, but add \"s\" to denote we're now working with smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate survivorship curves for singers, writers, and poets using *smoothed data*\n",
    "\n",
    "s_poet_survivors = []\n",
    "s_poet_survivor_percentages = []\n",
    "s_singer_survivors = []\n",
    "s_singer_survivor_percentages = []\n",
    "s_writer_survivors = []\n",
    "s_writer_survivor_percentages = []\n",
    "\n",
    "s_total_poets = s_poet_lifespans.size      # total number of avg data in poets sample\n",
    "s_total_singers = s_singer_lifespans.size  # total number of avg data in singers sample \n",
    "s_total_writers = s_writer_lifespans.size  # total number of avg data in writers sample\n",
    "\n",
    "for age in range(0, poet_lifespans.max().item()):\n",
    "    s_poet_survivors = s_poet_survivors + [s_poet_lifespans[s_poet_lifespans >= age].dropna().size]   \n",
    "    s_poet_survivor_percentages = s_poet_survivor_percentages + [s_poet_survivors[age]/s_total_poets]\n",
    "    \n",
    "    s_singer_survivors = s_singer_survivors + [s_singer_lifespans[s_singer_lifespans >= age].dropna().size]\n",
    "    s_singer_survivor_percentages = s_singer_survivor_percentages + [s_singer_survivors[age]/s_total_singers]\n",
    "    \n",
    "    s_writer_survivors = s_writer_survivors + [s_writer_lifespans[s_writer_lifespans >= age].dropna().size]\n",
    "    s_writer_survivor_percentages = s_writer_survivor_percentages + [s_writer_survivors[age]/s_total_writers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "survivorship_percentages_smoothed = pd.DataFrame(\n",
    "                            {\"age\": current_age,\n",
    "                            \"poet survivor %\": poet_survivor_percentages,\n",
    "                            \"singer survivor %\": singer_survivor_percentages, \n",
    "                            \"writer survivor %\": writer_survivor_percentages,\n",
    "                            \"poet survivor % (smoothed)\": s_poet_survivor_percentages,\n",
    "                            \"singer survivor % (smoothed)\": s_singer_survivor_percentages, \n",
    "                            \"writer survivor % (smoothed)\": s_writer_survivor_percentages,\n",
    "                            })\n",
    "\n",
    "survivorship_percentages_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our smoothed curves with our original curves! (Note we make these graphs a little bigger to facilitate visual inspection.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivorship_percentages_smoothed.plot(x=\"age\", y=[\"poet survivor %\", \"poet survivor % (smoothed)\"], \n",
    "                                       figsize=[13,10], title=\"Survivorship for poets, smoothed and original\", \n",
    "                                       grid =True)\n",
    "plt.ylabel('percentage still living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "survivorship_percentages_smoothed.plot(x=\"age\",  y=[\"singer survivor %\", \"singer survivor % (smoothed)\"], \n",
    "                                       figsize=[13,10], title=\"Survivorship for singers, smoothed and original\", \n",
    "                                       grid =True)\n",
    "plt.ylabel('percentage still living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivorship_percentages_smoothed.plot(x=\"age\",  y=[\"writer survivor %\", \"writer survivor % (smoothed)\"], \n",
    "                                       figsize=[13,10], title=\"Survivorship for writers, smoothed and original\", \n",
    "                                       grid =True)\n",
    "plt.ylabel('percentage still living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear from visual inspection that survivor curve for the smoothed poets' data is much smoother than our original data. Ditto for our writers. And the survivor curve for our singers became *less* smooth, not more. <b>What's going on here?</b> (HINT: There's no error in the code...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
